{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MTH8408 : Méthodes d'optimisation et contrôle optimal\n",
    " ## Laboratoire 3: Optimisation sans contraintes et méthodes itératives\n",
    "Tangi Migot et Paul Raynaud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, NLPModels, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNLPModel - Model with automatic differentiation backend ADModelBackend{\n",
       "  ForwardDiffADGradient,\n",
       "  ForwardDiffADHvprod,\n",
       "  EmptyADbackend,\n",
       "  EmptyADbackend,\n",
       "  EmptyADbackend,\n",
       "  ForwardDiffADHessian,\n",
       "  EmptyADbackend,\n",
       "}\n",
       "  Problem name: Generic\n",
       "   All variables: ████████████████████ 2      All constraints: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            free: ████████████████████ 2                 free: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "          infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            nnzh: (  0.00% sparsity)   3               linear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                    nonlinear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                         nnzj: (------% sparsity)         \n",
       "\n",
       "  Counters:\n",
       "             obj: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 grad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 cons: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "        cons_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0             cons_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 jcon: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jgrad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                  jac: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              jac_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         jac_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                jprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0            jprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "       jprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jtprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0           jtprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "      jtprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 hess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                hprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jhess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jhprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test problem:\n",
    "using ADNLPModels\n",
    "fH(x) = (x[2]+x[1].^2-11).^2+(x[1]+x[2].^2-7).^2\n",
    "x0H = [10., 20.]\n",
    "himmelblau = ADNLPModel(fH, x0H)\n",
    "\n",
    "problem2 = ADNLPModel(x->-x[1]^2, ones(3))\n",
    "\n",
    "roz(x) = 100 *  (x[2] - x[1]^2)^2 + (x[1] - 1.0)^2\n",
    "rosenbrock = ADNLPModel(roz, [-1.2, 1.0])\n",
    "\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "pb_du_cours = ADNLPModel(f, [-1.001, -1.001]) #ou [1.5, .5] ou [.5, .5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaires sur Julia\n",
    "\n",
    "Quelques commentaires sur des morceaux de codes que vous avez vu:\n",
    "- les structures, exemple [GenericExecutionStats](https://github.com/JuliaSmoothOptimizers/SolverCore.jl/blob/0091f437a26a27ac8aa53d5e37647223722f7f7c/src/stats.jl#L60) (constructeur, attribut, type).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SolverCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: invalid identifier name \"?\"",
     "output_type": "error",
     "traceback": [
      "syntax: invalid identifier name \"?\"\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Ulrizpascuit\\Desktop\\MTH8408\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? GenericExecutionStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les arguments dans les fonctions. Lire attentivement [la documentation Julia sur les fonctions](https://docs.julialang.org/en/v1/manual/functions/) pour comprendre l'utilisation des `Optional Arguments` et des `Keywords Arguments`. Ce type d'arguments est très utile dans nos applictions où les solveurs dépendent de paramètre dont on peut fixer des valeurs par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: Méthode BFGS avec mémoire limitée (L-BFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cet exercice est d'implémenter la méthode BFGS à mémoire limitée vue en cours en utilisant les `InverseLBFGSOperator` du package `LinearOperators.jl`. Il y a aussi un petit exemple dans la documentation du package [LinearOperators.jl/dev/tutorial/#Limited-memory-BFGS-and-SR1](https://juliasmoothoptimizers.github.io/LinearOperators.jl/dev/tutorial/#Limited-memory-BFGS-and-SR1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearOperators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: invalid identifier name \"?\"",
     "output_type": "error",
     "traceback": [
      "syntax: invalid identifier name \"?\"\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Ulrizpascuit\\Desktop\\MTH8408\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? InverseLBFGSOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ce qui est important dans ce type de méthode est:\n",
    "- le paramètre mémoire\n",
    "- la mise à jour de l'opérateur avec la fonction `push!`\n",
    "- si on a pas une direction de descente, alors on skip\n",
    "- recherche linéaire d'Armijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: invalid identifier name \"?\"",
     "output_type": "error",
     "traceback": [
      "syntax: invalid identifier name \"?\"\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Ulrizpascuit\\Desktop\\MTH8408\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? LinearOperators.push!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function armijo(xk, dk, fk, gk, slope, nlp :: AbstractNLPModel; τ1 = 1.0e-4, t_update = 1.5)\n",
    "  t = 1.0\n",
    "  fk_new = obj(nlp, xk + dk) # t = 1.0\n",
    "  while fk_new > fk + τ1 * t * slope\n",
    "    t /= t_update\n",
    "    fk_new = obj(nlp, xk + t * dk)\n",
    "  end\n",
    "  return t, fk_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "limited_bfgs (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function limited_bfgs(nlp      :: AbstractNLPModel;\n",
    "                      x        :: AbstractVector = nlp.meta.x0,\n",
    "                      atol     :: Real = √eps(eltype(x)), \n",
    "                      rtol     :: Real = √eps(eltype(x)),\n",
    "                      max_eval :: Int = -1,\n",
    "                      max_time :: Float64 = 30.0,\n",
    "                      f_min    :: Float64 = -1.0e16,\n",
    "                      verbose  :: Bool = true,\n",
    "                      mem      :: Int = 5)\n",
    "  start_time = time()\n",
    "  elapsed_time = 0.0\n",
    "  s=0\n",
    "  y=0\n",
    "  T = eltype(x)\n",
    "  n = nlp.meta.nvar\n",
    "\n",
    "  xt = zeros(T, n)\n",
    "  ∇ft = zeros(T, n)\n",
    "\n",
    "  f = obj(nlp, x)\n",
    "  ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "  H = InverseLBFGSOperator(T,n, ;mem=5, scaling=true)\n",
    "#################################################\n",
    "\n",
    "  ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "  ϵ = atol + rtol * ∇fNorm\n",
    "  iter = 0\n",
    "\n",
    "  @info log_header([:iter, :f, :dual, :slope, :bk], [Int, T, T, T, T],\n",
    "                   hdr_override=Dict(:f=>\"f(x)\", :dual=>\"‖∇f‖\", :slope=>\"∇fᵀd\"))\n",
    "\n",
    "  optimal = ∇fNorm ≤ ϵ\n",
    "  unbdd = f ≤ f_min\n",
    "  tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  stalled = false\n",
    "  status = :unknown\n",
    "\n",
    "  while !(optimal || tired || stalled || unbdd)\n",
    "\n",
    "#################################################\n",
    "    d = -H*∇f\n",
    "#################################################\n",
    "    slope = dot(d, ∇f)\n",
    "    if slope ≥ 0\n",
    "      @error \"not a descent direction\" slope\n",
    "      status = :not_desc\n",
    "      stalled = true\n",
    "      continue\n",
    "    end\n",
    "\n",
    "    # Perform improved Armijo linesearch.\n",
    "    t, ft = armijo(x, d, f, ∇f, slope, nlp)\n",
    "        \n",
    "    @info log_row(Any[iter, f, ∇fNorm, slope, t])\n",
    "\n",
    "    # Update L-BFGS approximation.\n",
    "    s = t*d\n",
    "    xt = x + t * d\n",
    "    ∇ft = grad(nlp, xt) # grad!(nlp, xt, ∇ft)\n",
    "    y = ∇ft - ∇f\n",
    "#################################################\n",
    "    LinearOperators.push!(H, s, y)\n",
    "#################################################\n",
    "\n",
    "    # Move on.\n",
    "    x = xt\n",
    "    f = ft\n",
    "    ∇f = ∇ft\n",
    "\n",
    "    ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "    iter = iter + 1\n",
    "\n",
    "    optimal = ∇fNorm ≤ ϵ\n",
    "    unbdd = f ≤ f_min\n",
    "    elapsed_time = time() - start_time\n",
    "    tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  end\n",
    "  @info log_row(Any[iter, f, ∇fNorm])\n",
    "\n",
    "  if optimal\n",
    "    status = :first_order\n",
    "  elseif tired\n",
    "    if neval_obj(nlp) > max_eval ≥ 0\n",
    "      status = :max_eval\n",
    "    elseif elapsed_time > max_time\n",
    "      status = :max_time\n",
    "    end\n",
    "  elseif unbdd\n",
    "        status = :unbounded\n",
    "  end\n",
    "\n",
    "  return GenericExecutionStats(\n",
    "        nlp,\n",
    "        status=status,\n",
    "        solution=x,\n",
    "        objective=f,\n",
    "        dual_feas=∇fNorm,\n",
    "        iter=iter,\n",
    "        elapsed_time=elapsed_time,\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `C:\\Users\\Ulrizpascuit\\.julia\\registries\\General.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Ulrizpascuit\\Desktop\\MTH8408\\MTH8408-Hiv24\\lab3\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Ulrizpascuit\\Desktop\\MTH8408\\MTH8408-Hiv24\\lab3\\Manifest.toml`"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Ulrizpascuit\\Desktop\\MTH8408\\MTH8408-Hiv24\\lab3\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Ulrizpascuit\\Desktop\\MTH8408\\MTH8408-Hiv24\\lab3\\Manifest.toml`"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Unit/Validation Tests\n",
    "# Réaliser un test unitaire\n",
    "using Pkg; Pkg.add(\"Test\"); Pkg.add(\"Logging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(stats.status, stats.solution) = (:first_order, [3.584428266659278, -1.8481265666485827])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(:first_order, [3.584428266659278, -1.8481265666485827])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "using Logging, Test\n",
    "stats = with_logger(NullLogger()) do \n",
    "    limited_bfgs(himmelblau) \n",
    "end\n",
    "@test stats.status == :first_order\n",
    "@test stats.solution ≈ [3.584428266659278, -1.8481265666485827] atol = 1e-6\n",
    "@show (stats.status, stats.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "\n",
    "- Compare l'implémentation de `limited_bfgs` avec la fonction `lbfgs` qui est disponible dans `JSOSolvers.jl`.\n",
    "- On veut pouvoir tester \"facilement\" plusieurs valeurs de $\\tau$ et du paramètre de mise à jour dans `armijo` sur les problèmes tests. Comment modifier le code pour que ça soit possible?\n",
    "\n",
    "On peut mesurer deux executions de fonctions Julia grâce aux fonctions de `BenchmarkTools.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: invalid identifier name \"?\"",
     "output_type": "error",
     "traceback": [
      "syntax: invalid identifier name \"?\"\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Ulrizpascuit\\Desktop\\MTH8408\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? @time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: NewtonCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cet exercice est d'adapter les méthodes de Newton de façon à résoudre le système linéaire avec une méthode itérative de type gradient conjugué comme suit ($B_k$ représente la matrice hessienne):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LineSearchNewtonCG.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cg_optim (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function cg_optim(H, ∇f)\n",
    "    #setup the tolerance:\n",
    "    n∇f = norm(∇f)\n",
    "    #####################################\n",
    "    ϵk = min(0.5, sqrt(n∇f))*n∇f\n",
    "    #################################### \n",
    "    n = length(∇f)\n",
    "    z = zeros(n)\n",
    "    r = ∇f\n",
    "    d = -r\n",
    "    \n",
    "    j = 0\n",
    "    while norm(r) ≥ ϵk && j < 3 * n\n",
    "     ###############################################\n",
    "        if dot(d, H * d) ≤ 0\n",
    "            if j == 0\n",
    "                p = -∇f\n",
    "            else\n",
    "                p = z\n",
    "            end\n",
    "        end\n",
    "     ##############################################\n",
    "        α = r' * r / (d' * H * d)\n",
    "     ##############################################        \n",
    "        z += α * d\n",
    "        nrr2 = dot(r, r)\n",
    "        r += α * H * d\n",
    "     ##############################################\n",
    "        β  = r' * r / nrr2\n",
    "     ##############################################\n",
    "        d  = -r + β * d\n",
    "        j += 1\n",
    "   end\n",
    "   return z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: extra token \"def\" after end of expression",
     "output_type": "error",
     "traceback": [
      "syntax: extra token \"def\" after end of expression\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Ulrizpascuit\\Desktop\\MTH8408\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "matrice def posi 4x4\n",
    "resoudre Hz=gradf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui est important ici est qu'on a pas besoin de stocker/évaluer la matrice hessienne entière mais simplement le produit entre la hessienne et un vecteur. Pour un `NLPModels` on utilise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: invalid identifier name \"?\"",
     "output_type": "error",
     "traceback": [
      "syntax: invalid identifier name \"?\"\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Ulrizpascuit\\Desktop\\MTH8408\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? NLPModels.hprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: invalid identifier name \"?\"",
     "output_type": "error",
     "traceback": [
      "syntax: invalid identifier name \"?\"\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Ulrizpascuit\\Desktop\\MTH8408\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? NLPModels.hess_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo_Newton_cg (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function armijo_Newton_cg(nlp      :: AbstractNLPModel;\n",
    "                          x        :: AbstractVector = nlp.meta.x0,\n",
    "                          atol     :: Real = √eps(eltype(x)), \n",
    "                          rtol     :: Real = √eps(eltype(x)),\n",
    "                          max_eval :: Int = -1,\n",
    "                          max_time :: Float64 = 30.0,\n",
    "                          f_min    :: Float64 = -1.0e16)\n",
    "  start_time = time()\n",
    "  elapsed_time = 0.0\n",
    "\n",
    "  T = eltype(x)\n",
    "  n = nlp.meta.nvar\n",
    "\n",
    "  f = obj(nlp, x)\n",
    "  ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "  H = NLPModels.hess_op(nlp,x)\n",
    "#################################################\n",
    "\n",
    "  ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "  ϵ = atol + rtol * ∇fNorm\n",
    "  iter = 0\n",
    "\n",
    "  @info log_header([:iter, :f, :dual, :slope, :bk], [Int, T, T, T, T],\n",
    "                   hdr_override=Dict(:f=>\"f(x)\", :dual=>\"‖∇f‖\", :slope=>\"∇fᵀd\"))\n",
    "\n",
    "  optimal = ∇fNorm ≤ ϵ\n",
    "  unbdd = f ≤ f_min\n",
    "  tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  stalled = false\n",
    "  status = :unknown\n",
    "\n",
    "  while !(optimal || tired || stalled || unbdd)\n",
    "        \n",
    "    d = cg_optim(H, ∇f)\n",
    "        \n",
    "    slope = dot(d, ∇f)\n",
    "    if slope ≥ 0\n",
    "      @error \"not a descent direction\" slope\n",
    "      status = :not_desc\n",
    "      stalled = true\n",
    "      continue\n",
    "    end\n",
    "\n",
    "    # Perform improved Armijo linesearch.\n",
    "    t, f = armijo(x, d, f, ∇f, slope, nlp)\n",
    "        \n",
    "    @info log_row(Any[iter, f, ∇fNorm, slope, t])\n",
    "\n",
    "    # Update L-BFGS approximation.\n",
    "    x += t * d\n",
    "    ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "    H = NLPModels.hess_op(nlp,x)\n",
    "#################################################\n",
    "\n",
    "    ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "    iter = iter + 1\n",
    "\n",
    "    optimal = ∇fNorm ≤ ϵ\n",
    "    unbdd = f ≤ f_min\n",
    "    elapsed_time = time() - start_time\n",
    "    tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  end\n",
    "  @info log_row(Any[iter, f, ∇fNorm])\n",
    "\n",
    "  if optimal\n",
    "    status = :first_order\n",
    "  elseif tired\n",
    "    if neval_obj(nlp) > max_eval ≥ 0\n",
    "      status = :max_eval\n",
    "    elseif elapsed_time > max_time\n",
    "      status = :max_time\n",
    "    end\n",
    "  elseif unbdd\n",
    "        status = :unbounded\n",
    "  end\n",
    "\n",
    "  return GenericExecutionStats(nlp, status = status, solution=x, objective=f, dual_feas=∇fNorm,\n",
    "                               iter=iter, elapsed_time=elapsed_time)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(stats.status, stats.solution) = (:first_order, [1.000000000013107, 6.553559582523889e-12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(:first_order, [1.000000000013107, 6.553559582523889e-12])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Unit/Validation Tests\n",
    "# Réaliser un test unitaire\n",
    "stats = with_logger(NullLogger()) do \n",
    "    armijo_Newton_cg(pb_du_cours, x = [1.5, .5]) \n",
    "end\n",
    "@test stats.status == :first_order\n",
    "@test stats.solution ≈ [1., 0.] atol = 1e-6\n",
    "@show (stats.status, stats.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment préparer un benchmark\n",
    "\n",
    "On veut maintenant pouvoir réaliser un benchmark de plusieurs solveurs. Pour comparer les algorithmes, il nous faut une collection de problèmes tests et on va utiliser `OptimizationProblems.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "using ADNLPModels\n",
    "using OptimizationProblems, OptimizationProblems.ADNLPProblems\n",
    "using LinearAlgebra\n",
    "using SolverCore, SolverBenchmark\n",
    "using ADNLPModels, NLPModels\n",
    "using OptimizationProblems, OptimizationProblems.ADNLPProblems\n",
    "using JSOSolvers\n",
    "\n",
    "first_order(df) = df.status .== :first_order\n",
    "unbounded(df) = df.status .== :unbounded\n",
    "solved(df) = first_order(df) .| unbounded(df)\n",
    "solved(ges)\n",
    "\n",
    "n = 20\n",
    "solvers = solvers = Dict(\n",
    "  :lbfgs => model -> lbfgs(model, mem=5),\n",
    "  :armijo_Newton_cg  => model -> armijo_Newton_cg(model),\n",
    "  # :trunk => model -> trunk(model),\n",
    ")\n",
    "\n",
    "ad_problems = (eval(Meta.parse(problem))(;n) for problem ∈ OptimizationProblems.meta[!, :name])\n",
    "\n",
    "stats = bmark_solvers(\n",
    "  solvers, ad_problems,\n",
    "  skipif=prob -> (!unconstrained(prob) || get_nvar(prob) > 100 || get_nvar(prob) < 5),\n",
    ")\n",
    "\n",
    "costnames = [\"time\", \"obj + grad + hess\"]\n",
    "costs = [\n",
    "  df -> .!solved(df) .* Inf .+ df.elapsed_time,\n",
    "  df -> .!solved(df) .* Inf .+ df.neval_obj .+ df.neval_grad .+ df.neval_hess,\n",
    "]\n",
    "\n",
    "using Plots\n",
    "gr()\n",
    "\n",
    "profile_solvers(stats, costs, costnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [:id, :name, :nvar, :objective, :dual_feas, :neval_obj, :neval_grad, :neval_hess, :iter, :elapsed_time, :status]\n",
    "header = Dict(\n",
    "  :nvar => \"n\",\n",
    "  :objective => \"f(x)\",\n",
    "  :dual_feas => \"‖∇f(x)‖\",\n",
    "  :neval_obj => \"# f\",\n",
    "  :neval_grad => \"# ∇f\",\n",
    "  :neval_hess => \"# ∇²f\",\n",
    "  :elapsed_time => \"t\",\n",
    ")\n",
    "\n",
    "for solver ∈ keys(solvers)\n",
    "  pretty_stats(stats[solver][!, cols], hdr_override=header)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
